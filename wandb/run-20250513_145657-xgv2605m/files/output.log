
Total parameters: 322693
  0%|[34m                                                                                                                                                                       [39m| 0/52 [00:00<?, ?it/s]
Epoch 1/150
  0%|[34m                                                                                                                                                                       [39m| 0/52 [00:06<?, ?it/s]
Traceback (most recent call last):
  File "D:\IIT Delhi\Projects\Adversarial games\Experimentation\codeBase\trainer.py", line 86, in <module>
    train_metrics, val_metrics = train_val(train_dataLoader,
  File "D:\IIT Delhi\Projects\Adversarial games\Experimentation\codeBase\epoch.py", line 155, in train_val
    loss_hgr_train_curr, loss_id_train_curr, loss_icgd_train_curr, loss_train_curr, acc_hgr_train_curr, acc_id_train_curr = train_epoch(train_loader,
  File "D:\IIT Delhi\Projects\Adversarial games\Experimentation\codeBase\epoch.py", line 52, in train_epoch
    args.lambda_id*loss_id_batch.backward()
  File "C:\Users\ASUS\anaconda3\envs\gpu_torch_6\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\ASUS\anaconda3\envs\gpu_torch_6\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.